{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d653fc-bd6d-4dde-8850-a7ad2f262f72",
   "metadata": {},
   "source": [
    "# Notes\n",
    "1. I did not have the time to make the preprocessing steps in one class so I used 9 functions.\n",
    "2. The preprocessing steps have 3 cells \n",
    "3. There is a function on the __2nd preprocessing cell__ that translates texts (it detected 60 texts but only 20 were non english).\n",
    "4. If it is considered prohibited you can exclude (it does not change the cross validation accuracy at all). I added a Note to differentiate easier.\n",
    "\n",
    "__Important:__ It is mentioned on the moodle to create a function named train_test(train.csv, test.csv). I was confused , and I just read train.csv and test.csv in a pandas dataframe, considering that you will add test.csv on the folder that i will submit which contains my .pynb files and train.csv. If you don't add  test.csv on the folder that i will give you, please change the path on train and test dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1236f03c-5798-46d7-aa4c-00940be6d582",
   "metadata": {},
   "source": [
    "# Install the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d4fe6e6-62de-4398-a651-79f2039906f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "bea5c9bf-4e60-4fac-b963-987a481cb61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordninja  #this splits words and is used only on hashtags e.g #BestHolidays -> Best Holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3437de-b008-47b4-ad38-8830366aa15e",
   "metadata": {},
   "source": [
    "##### Don't install these if translating is considered prohibited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6ce79e-ce6c-4e2c-9eda-cbc340d2e09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langdetect\n",
    "#pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4d50db-5010-4b88-9d2b-dcacf1701f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import wordninja\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc0a539-557a-40c1-a4b7-17ed3ad8896c",
   "metadata": {},
   "source": [
    "# Read train.csv and test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9495ef1-058e-4eb4-9f89-104dfba610c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# If train.csv and test.csv are added on the folder with this notebook\u001b[39;00m\n\u001b[0;32m      2\u001b[0m train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test.csv'"
     ]
    }
   ],
   "source": [
    "# If train.csv and test.csv are added on the folder with this notebook\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ed37d-85bf-49d2-a6ce-7eabdb29ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a72fc1f-ba2c-49a8-8a70-e9a935110d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a774144e-4f40-4a72-83b6-c6b3ad6b6dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3cf96c-7e0a-4827-ab6b-5b7dfa41105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d678de00-3518-4f5e-a87d-4363f848bf11",
   "metadata": {},
   "source": [
    "#### If the test does not have the same column names with train please rename it to text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1e8298-aa3a-421d-abcc-771368a02cd5",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb1e76-bd67-4a70-bd38-dfdf6503291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL_HTML(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http.\\S+\", '', text)\n",
    "    text = re.sub(r\"www.\\S+\", '', text)\n",
    "    text = re.sub(r\"pic.twitter.com\\S+\", '', text)\n",
    "    # Remove HTML/XML tags\n",
    "    if re.search(r'<.*?>', text):  \n",
    "        text = BeautifulSoup(text, \"html.parser\").get_text()  \n",
    "        \n",
    "    return text\n",
    "\n",
    "train['cleaned_text'] = train['text'].apply(remove_URL_HTML)\n",
    "test['cleaned_text'] = test['text'].apply(remove_URL_HTML)\n",
    "\n",
    "def remove_tags_and_split_hashtags(text, keep_hashtags=True):\n",
    "    # Remove @USER (e.g., @user1, @someone)\n",
    "    text = re.sub(r'@[\\w]+', '', text)\n",
    "\n",
    "    if keep_hashtags:\n",
    "        # Find all hashtags (e.g., #something)\n",
    "        hashtags = re.findall(r'#\\w+', text)\n",
    "        \n",
    "        # Replace hashtags with their split version\n",
    "        for hashtag in hashtags:\n",
    "            # Remove the '#' and split the word into components\n",
    "            split_words = ' '.join(wordninja.split(hashtag[1:]))\n",
    "            text = text.replace(hashtag, split_words)\n",
    "    else:\n",
    "        # Remove hashtags completely\n",
    "        text = re.sub(r'#\\w+', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "train['cleaned_text'] = train['cleaned_text'].apply(lambda x: remove_tags_and_split_hashtags(x, keep_hashtags=True))\n",
    "test['cleaned_text'] = test['cleaned_text'].apply(lambda x: remove_tags_and_split_hashtags(x, keep_hashtags=True))\n",
    "\n",
    "# This is a custom dict\n",
    "emoji_dict = {\n",
    "    r':-\\)': ' happy face ',\n",
    "    r':\\)': ' happy face ',\n",
    "    r':D': ' laugh face ',\n",
    "    r':-\\(': ' sad face ',\n",
    "    r':\\(': ' sad face ',\n",
    "    r':O': ' surprised face ',\n",
    "    r';-\\)': ' wink face ',\n",
    "    r';\\)': ' wink face ',\n",
    "    r':P': ' playful face ',\n",
    "    r':-\\?': ' undecided face ',\n",
    "    r':/': ' undecided face ',\n",
    "    r\"\\b't\\b\": \" not\" # I added this to keep all the verbs with not e.g can't -> cannot\n",
    "}  \n",
    "\n",
    "def replace_patterns_with_meaning(text, emoji_dict, replace=True):\n",
    "    if replace:\n",
    "        for emoji, meaning in emoji_dict.items():\n",
    "            text = re.sub(emoji, meaning, text)\n",
    "    return text\n",
    "\n",
    "train['cleaned_text'] = train['cleaned_text'].apply(lambda x: replace_patterns_with_meaning(x, emoji_dict))\n",
    "test['cleaned_text'] = test['cleaned_text'].apply(lambda x: replace_patterns_with_meaning(x, emoji_dict))\n",
    "\n",
    "# This is from Kaggle I added this on Report's references.\n",
    "abbreviations = {\n",
    "    \"$\" : \" dollar \",\n",
    "    \"€\" : \" euro \",\n",
    "    \"4ao\" : \"for adults only\",\n",
    "    \"a.m\" : \"before midday\",\n",
    "    \"a3\" : \"anytime anywhere anyplace\",\n",
    "    \"aamof\" : \"as a matter of fact\",\n",
    "    \"acct\" : \"account\",\n",
    "    \"adih\" : \"another day in hell\",\n",
    "    \"afaic\" : \"as far as i am concerned\",\n",
    "    \"afaict\" : \"as far as i can tell\",\n",
    "    \"afaik\" : \"as far as i know\",\n",
    "    \"afair\" : \"as far as i remember\",\n",
    "    \"afk\" : \"away from keyboard\",\n",
    "    \"app\" : \"application\",\n",
    "    \"approx\" : \"approximately\",\n",
    "    \"apps\" : \"applications\",\n",
    "    \"asap\" : \"as soon as possible\",\n",
    "    \"asl\" : \"age, sex, location\",\n",
    "    \"atk\" : \"at the keyboard\",\n",
    "    \"ave.\" : \"avenue\",\n",
    "    \"af\" : \"as fuck\",\n",
    "    \"aymm\" : \"are you my mother\",\n",
    "    \"ayor\" : \"at your own risk\", \n",
    "    \"b&b\" : \"bed and breakfast\",\n",
    "    \"b+b\" : \"bed and breakfast\",\n",
    "    \"b.c\" : \"before christ\",\n",
    "    \"b2b\" : \"business to business\",\n",
    "    \"b2c\" : \"business to customer\",\n",
    "    \"b4\" : \"before\",\n",
    "    \"b4n\" : \"bye for now\",\n",
    "    \"b@u\" : \"back at you\",\n",
    "    \"bae\" : \"before anyone else\",\n",
    "    \"bak\" : \"back at keyboard\",\n",
    "    \"bbbg\" : \"bye bye be good\",\n",
    "    \"bbc\" : \"british broadcasting corporation\",\n",
    "    \"bbias\" : \"be back in a second\",\n",
    "    \"bbl\" : \"be back later\",\n",
    "    \"bbs\" : \"be back soon\",\n",
    "    \"be4\" : \"before\",\n",
    "    \"bfn\" : \"bye for now\",\n",
    "    \"blvd\" : \"boulevard\",\n",
    "    \"bout\" : \"about\",\n",
    "    \"brb\" : \"be right back\",\n",
    "    \"bros\" : \"brothers\",\n",
    "    \"brt\" : \"be right there\",\n",
    "    \"bsaaw\" : \"big smile and a wink\",\n",
    "    \"btw\" : \"by the way\",\n",
    "    \"bwl\" : \"bursting with laughter\",\n",
    "    \"c/o\" : \"care of\",\n",
    "    \"cet\" : \"central european time\",   \n",
    "    \"cf\" : \"compare\",\n",
    "    \"cia\" : \"central intelligence agency\",\n",
    "    \"csl\" : \"can not stop laughing\",\n",
    "    \"cu\" : \"see you\",\n",
    "    \"cul8r\" : \"see you later\",\n",
    "    \"cv\" : \"curriculum vitae\",\n",
    "    \"cwot\" : \"complete waste of time\",\n",
    "    \"cya\" : \"see you\",\n",
    "    \"cyt\" : \"see you tomorrow\",\n",
    "    \"c'mon\" : \"come on\",\n",
    "    \"dae\" : \"does anyone else\",\n",
    "    \"dbmib\" : \"do not bother me i am busy\",\n",
    "    \"diy\" : \"do it yourself\",\n",
    "    \"dm\" : \"direct message\",\n",
    "    \"dwh\" : \"during work hours\",\n",
    "    \"e123\" : \"easy as one two three\",\n",
    "    \"eet\" : \"eastern european time\",\n",
    "    \"eg\" : \"example\",\n",
    "    \"embm\" : \"early morning business meeting\",\n",
    "    \"encl\" : \"enclosed\",\n",
    "    \"encl.\" : \"enclosed\",\n",
    "    \"etc\" : \"and so on\",\n",
    "    \"faq\" : \"frequently asked questions\",\n",
    "    \"fawc\" : \"for anyone who cares\",\n",
    "    \"fb\" : \"facebook\",\n",
    "    \"fc\" : \"fingers crossed\",\n",
    "    \"fig\" : \"figure\",\n",
    "    \"fimh\" : \"forever in my heart\", \n",
    "    \"ft.\" : \"feet\",\n",
    "    \"ft\" : \"featuring\",\n",
    "    \"ftl\" : \"for the loss\",\n",
    "    \"ftw\" : \"for the win\",\n",
    "    \"fwiw\" : \"for what it is worth\",\n",
    "    \"fyi\" : \"for your information\",\n",
    "    \"g9\" : \"genius\",\n",
    "    \"gahoy\" : \"get a hold of yourself\",\n",
    "    \"gal\" : \"get a life\",\n",
    "    \"gcse\" : \"general certificate of secondary education\",\n",
    "    \"gfn\" : \"gone for now\",\n",
    "    \"gg\" : \"good game\",\n",
    "    \"gl\" : \"good luck\",\n",
    "    \"glhf\" : \"good luck have fun\",\n",
    "    \"gmt\" : \"greenwich mean time\",\n",
    "    \"gmta\" : \"great minds think alike\",\n",
    "    \"gn\" : \"good night\",\n",
    "    \"g.o.a.t\" : \"greatest of all time\",\n",
    "    \"goat\" : \"greatest of all time\",\n",
    "    \"goi\" : \"get over it\",\n",
    "    \"gps\" : \"global positioning system\",\n",
    "    \"gr8\" : \"great\",\n",
    "    \"gratz\" : \"congratulations\",\n",
    "    \"gyal\" : \"girl\",\n",
    "    \"h&c\" : \"hot and cold\",\n",
    "    \"hp\" : \"horsepower\",\n",
    "    \"hr\" : \"hour\",\n",
    "    \"hrh\" : \"his royal highness\",\n",
    "    \"ht\" : \"height\",\n",
    "    \"ibrb\" : \"i will be right back\",\n",
    "    \"ic\" : \"i see\",\n",
    "    \"icq\" : \"i seek you\",\n",
    "    \"icymi\" : \"in case you missed it\",\n",
    "    \"idc\" : \"i do not care\",\n",
    "    \"idgadf\" : \"i do not give a damn fuck\",\n",
    "    \"idgaf\" : \"i do not give a fuck\",\n",
    "    \"idk\" : \"i do not know\",\n",
    "    \"ie\" : \"that is\",\n",
    "    \"i.e\" : \"that is\",\n",
    "    \"ifyp\" : \"i feel your pain\",\n",
    "    \"IG\" : \"instagram\",\n",
    "    \"iirc\" : \"if i remember correctly\",\n",
    "    \"ilu\" : \"i love you\",\n",
    "    \"ily\" : \"i love you\",\n",
    "    \"imho\" : \"in my humble opinion\",\n",
    "    \"imo\" : \"in my opinion\",\n",
    "    \"imu\" : \"i miss you\",\n",
    "    \"iow\" : \"in other words\",\n",
    "    \"irl\" : \"in real life\",\n",
    "    \"j4f\" : \"just for fun\",\n",
    "    \"jic\" : \"just in case\",\n",
    "    \"jk\" : \"just kidding\",\n",
    "    \"jsyk\" : \"just so you know\",\n",
    "    \"l8r\" : \"later\",\n",
    "    \"lb\" : \"pound\",\n",
    "    \"lbs\" : \"pounds\",\n",
    "    \"ldr\" : \"long distance relationship\",\n",
    "    \"lmao\" : \"laugh my ass off\",\n",
    "    \"lmfao\" : \"laugh my fucking ass off\",\n",
    "    \"lol\" : \"laughing out loud\",\n",
    "    \"ltd\" : \"limited\",\n",
    "    \"ltns\" : \"long time no see\",\n",
    "    \"m8\" : \"mate\",\n",
    "    \"mf\" : \"motherfucker\",\n",
    "    \"mfs\" : \"motherfuckers\",\n",
    "    \"mfw\" : \"my face when\",\n",
    "    \"mofo\" : \"motherfucker\",\n",
    "    \"mph\" : \"miles per hour\",\n",
    "    \"mr\" : \"mister\",\n",
    "    \"mrw\" : \"my reaction when\",\n",
    "    \"ms\" : \"miss\",\n",
    "    \"mte\" : \"my thoughts exactly\",\n",
    "    \"nagi\" : \"not a good idea\",\n",
    "    \"nbc\" : \"national broadcasting company\",\n",
    "    \"nbd\" : \"not big deal\",\n",
    "    \"nfs\" : \"not for sale\",\n",
    "    \"ngl\" : \"not going to lie\",\n",
    "    \"nhs\" : \"national health service\",\n",
    "    \"nrn\" : \"no reply necessary\",\n",
    "    \"nsfl\" : \"not safe for life\",\n",
    "    \"nsfw\" : \"not safe for work\",\n",
    "    \"nth\" : \"nice to have\",\n",
    "    \"nvr\" : \"never\",\n",
    "    \"nyc\" : \"new york city\",\n",
    "    \"oc\" : \"original content\",\n",
    "    \"og\" : \"original\",\n",
    "    \"ohp\" : \"overhead projector\",\n",
    "    \"oic\" : \"oh i see\",\n",
    "    \"omdb\" : \"over my dead body\",\n",
    "    \"omg\" : \"oh my god\",\n",
    "    \"omw\" : \"on my way\",\n",
    "    \"p.a\" : \"per annum\",\n",
    "    \"p.m\" : \"after midday\",\n",
    "    \"pm\" : \"prime minister\",\n",
    "    \"poc\" : \"people of color\",\n",
    "    \"pov\" : \"point of view\",\n",
    "    \"pp\" : \"pages\",\n",
    "    \"ppl\" : \"people\",\n",
    "    \"prw\" : \"parents are watching\",\n",
    "    \"ps\" : \"postscript\",\n",
    "    \"pt\" : \"point\",\n",
    "    \"ptb\" : \"please text back\",\n",
    "    \"pto\" : \"please turn over\",\n",
    "    \"qpsa\" : \"what happens\", #\"que pasa\",\n",
    "    \"ratchet\" : \"rude\",\n",
    "    \"rbtl\" : \"read between the lines\",\n",
    "    \"rlrt\" : \"real life retweet\", \n",
    "    \"rofl\" : \"rolling on the floor laughing\",\n",
    "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "    \"rt\" : \"retweet\",\n",
    "    \"ruok\" : \"are you ok\",\n",
    "    \"sfw\" : \"safe for work\",\n",
    "    \"sk8\" : \"skate\",\n",
    "    \"smh\" : \"shake my head\",\n",
    "    \"sq\" : \"square\",\n",
    "    \"srsly\" : \"seriously\", \n",
    "    \"ssdd\" : \"same stuff different day\",\n",
    "    \"tbh\" : \"to be honest\",\n",
    "    \"tbs\" : \"tablespooful\",\n",
    "    \"tbsp\" : \"tablespooful\",\n",
    "    \"tfw\" : \"that feeling when\",\n",
    "    \"thks\" : \"thank you\",\n",
    "    \"tho\" : \"though\",\n",
    "    \"thx\" : \"thank you\",\n",
    "    \"tia\" : \"thanks in advance\",\n",
    "    \"til\" : \"today i learned\",\n",
    "    \"tl;dr\" : \"too long i did not read\",\n",
    "    \"tldr\" : \"too long i did not read\",\n",
    "    \"tmb\" : \"tweet me back\",\n",
    "    \"tntl\" : \"trying not to laugh\",\n",
    "    \"ttyl\" : \"talk to you later\",\n",
    "    \"u\" : \"you\",\n",
    "    \"u2\" : \"you too\",\n",
    "    \"u4e\" : \"yours for ever\",\n",
    "    \"utc\" : \"coordinated universal time\",\n",
    "    \"w/\" : \"with\",\n",
    "    \"w/o\" : \"without\",\n",
    "    \"w8\" : \"wait\",\n",
    "    \"wassup\" : \"what is up\",\n",
    "    \"wb\" : \"welcome back\",\n",
    "    \"wtf\" : \"what the fuck\",\n",
    "    \"wtg\" : \"way to go\",\n",
    "    \"wtpa\" : \"where the party at\",\n",
    "    \"wuf\" : \"where are you from\",\n",
    "    \"wuzup\" : \"what is up\",\n",
    "    \"wywh\" : \"wish you were here\",\n",
    "    \"yd\" : \"yard\",\n",
    "    \"ygtr\" : \"you got that right\",\n",
    "    \"ynk\" : \"you never know\",\n",
    "    \"zzz\" : \"sleeping bored and tired\"\n",
    "}\n",
    "\n",
    "def replace_abbreviations(text, abbreviations, replace=True):\n",
    "    if replace:\n",
    "        words = text.split()\n",
    "        replaced_words = [abbreviations.get(word.lower(), word) for word in words] \n",
    "    return \" \".join(replaced_words)\n",
    "\n",
    "train['cleaned_text'] = train['cleaned_text'].apply(lambda x: replace_abbreviations(x, abbreviations))\n",
    "test['cleaned_text'] = test['cleaned_text'].apply(lambda x: replace_abbreviations(x, abbreviations))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase, remove punctuation and extra spaces\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r'\\d+', ' ', text) \n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text) \n",
    "#    text = re.sub(r'[^!\\?\\'\\w\\s]', '', text)  # Keep '!', '?'??????????????? to capture more expressions??\n",
    "    return text\n",
    "\n",
    "train['cleaned_text'] = train['cleaned_text'].apply(preprocess_text)\n",
    "test['cleaned_text'] = test['cleaned_text'].apply(preprocess_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4047bbc-4549-4aa7-a75a-1d81edee6ae6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Note (translate to english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48855e67-68db-46d1-814c-9a684443ad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Skip this part if this is considered pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d3fcbf-faad-4700-ace2-ee6fa2e253b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "from langdetect import detect\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "# Function to detect language and translate to English\n",
    "def translate_to_english(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        if language != 'en':\n",
    "            translated_text = translator.translate(text, src=language, dest='en').text\n",
    "            return translated_text\n",
    "        else:\n",
    "            return text \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        #return original text in case of any error\n",
    "        return text\n",
    "\n",
    "\n",
    "train['cleaned_text'] = train['cleaned_text'].apply(translate_to_english)\n",
    "test['cleaned_text'] = test['cleaned_text'].apply(translate_to_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431bee35-2544-41bc-810d-51fa441f0847",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6368ad-68da-4bf6-84a9-479b876dada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_and_stem(text, stopwords=False):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = word_tokenize(text)  \n",
    "    if stopwords:\n",
    "        stemmed_tokens = [stemmer.stem(word.lower()) for word in tokens if word.lower() not in stop_words]\n",
    "    else:\n",
    "        stemmed_tokens = [stemmer.stem(word.lower()) for word in tokens]\n",
    "    return \" \".join(stemmed_tokens)\n",
    "\n",
    "\n",
    "train['cleaned_text'] = train['cleaned_text'].apply(stopwords_and_stem)\n",
    "test['cleaned_text'] = test['cleaned_text'].apply(stopwords_and_stem)\n",
    "\n",
    "def remove_short_words(text):\n",
    "    \n",
    "    words = text.split()  \n",
    "    filtered_words = [word for word in words if len(word) >= 2]  \n",
    "    return ' '.join(filtered_words)  \n",
    "\n",
    "train['cleaned_text'] = train['cleaned_text'].apply(remove_short_words)\n",
    "test['cleaned_text'] = test['cleaned_text'].apply(remove_short_words)\n",
    "\n",
    "def preprocess_text_with_token_limit(text, token_limit=200, token_cut=100):\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    if len(tokens) > token_limit:\n",
    "        tokens = tokens[:token_cut] + tokens[-token_cut:]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "train['cleaned_text'] = train['cleaned_text'].apply(preprocess_text_with_token_limit)\n",
    "test['cleaned_text'] = test['cleaned_text'].apply(preprocess_text_with_token_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d9c362-e318-421f-b4d0-5ecd806afa7a",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0746fb-bc50-4c69-847d-90cc03acc222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import  VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8ff8d8-3ddf-4146-8686-e4b3cb4deab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalPowerNormalization(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, transform_power_norm=True):\n",
    "        self.transform_power_norm = transform_power_norm\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.transform_power_norm:\n",
    "            X.data **= 0.5  # Apply square root transformation \n",
    "            # Normalize the sparse matrix\n",
    "            normalize(X, copy=False)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6bb579-0326-44fe-86b2-d1f51e0a0022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipelines with best parameters of tfi-df/countervec for each model\n",
    "linear_svc_pipeline = make_pipeline(\n",
    "    TfidfVectorizer(max_df=0.3, max_features=None, min_df=1, ngram_range=(1, 2), norm='l1', sublinear_tf=False), \n",
    "    ConditionalPowerNormalization(transform_power_norm=True),\n",
    "    LinearSVC(C=0.5, class_weight='balanced', loss='squared_hinge')\n",
    ")\n",
    "\n",
    "multinomial_nb_pipeline = make_pipeline(\n",
    "    CountVectorizer(max_df=0.25, min_df=2, ngram_range=(1, 2)),  \n",
    "    MultinomialNB(alpha=1, fit_prior=True)\n",
    ")\n",
    "\n",
    "logistic_reg_pipeline = make_pipeline(\n",
    "    TfidfVectorizer(max_df=0.45, min_df=2, ngram_range=(1, 2), sublinear_tf=True),  \n",
    "    LogisticRegression(C=1, multi_class='multinomial', penalty='l2', solver='lbfgs')\n",
    ")\n",
    "\n",
    "# Voting classifier \n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('linear_svc', linear_svc_pipeline),\n",
    "    ('multinomial_nb', multinomial_nb_pipeline),\n",
    "    ('logistic_reg', logistic_reg_pipeline)\n",
    "], voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b0add-2d85-45cc-a0a5-298bdf89a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['cleaned_text']\n",
    "y_train = train['sentiment']\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb27f9-3ed6-4186-a887-d64789cbdc52",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1256c5e4-1169-4e75-ae8c-d7763235923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test['cleaned_text']\n",
    "#y_test = test['sentiment']   \n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011f580c-3de7-49cc-a73b-7051e345d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = voting_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32012525-f2f1-4d0a-a673-6a4324f00049",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d39a61-dd68-40c6-9bb9-e7154ca7967a",
   "metadata": {},
   "source": [
    "# predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8121e7e6-be1a-470a-8e52-7882881178e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predictions.txt', 'w') as f:\n",
    "    for pred in predictions:\n",
    "        f.write(str(pred) + '\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a7514b-a21c-4eb4-bc5d-806e4074dc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a243e6-86e9-44cd-a1b3-c60549b7c7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27ed214-3457-4587-86c7-ca7eff1ca262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5647983-7376-4076-a53f-799f7b02269d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7c67e0-5490-4b96-ba2b-53789db1012b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc2b9595-3165-4aeb-8cf3-cbd5580672dc",
   "metadata": {},
   "source": [
    "### Ignore this i was validating the entire procedure on 20 custom texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b36690b2-bdcd-4846-b96c-e6a54417cbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test on 20 custom texts\n",
    "y_pred = voting_clf.predict(test['cleaned_text'])\n",
    "test_accuracy = accuracy_score(test['sentiment'], y_pred)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fe91c71c-97bc-4c46-a619-26bf4e48edb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['cleaned_text'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "15dedeb2-d902-43de-8ebf-7368521c9dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0a2405e6-0e37-4900-9dd5-416d036df1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred == predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99d160f-06c6-4ba2-a8d5-93044b31c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(voting_clf, X_train, y_train, cv=StratifiedKFold(n_splits=8, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1e060f46-69dc-48ef-a762-7b9647f71258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.603875"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "74a1f7c0-b282-4cfd-821d-65210510bd42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59 , 0.611, 0.613, 0.596, 0.63 , 0.583, 0.595, 0.613])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3e922-eb1b-4cb4-ad7c-c0b0add0d9df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
